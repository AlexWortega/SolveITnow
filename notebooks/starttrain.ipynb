{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2d254560-941f-4475-a66e-574e84f7173b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "550f6d4401ac4faea0be73f470797eea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f097e92a45943678b11e2a59f506f25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd8b19ea42ef4cf5b99426a74aab801a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"json\",data_files= \"leetcode-solutions.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "86eda651-c55d-48e2-a18d-4a2acdf03721",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['code_only', 'code_with_data', 'code_with_problem', 'id', 'explanation_only'],\n",
       "    num_rows: 2359\n",
       "})"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3e63594f-e8ce-42f5-b6fb-43cbe6f20d23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# You are given a **0-indexed** integer array `nums`. You can apply the following operation any number of times:\\n\\n*   Pick any element from `nums` and put it at the end of `nums`.\\n\\nThe prefix sum array of `nums` is an array `prefix` of the same length as `nums` such that `prefix[i]` is the sum of all the integers `nums[j]` where `j` is in the inclusive range `[0, i]`.\\n\\nReturn _the minimum number of operations such that the prefix sum array does not contain negative integers_. The test cases are generated such that it is always possible to make the prefix sum array non-negative.\\n\\n**Example 1:**\\n\\n**Input:** nums = \\\\[2,3,-5,4\\\\]\\n**Output:** 0\\n**Explanation:** we do not need to do any operations.\\nThe array is \\\\[2,3,-5,4\\\\]. The prefix sum array is \\\\[2, 5, 0, 4\\\\].\\n\\n**Example 2:**\\n\\n**Input:** nums = \\\\[3,-5,-2,6\\\\]\\n**Output:** 1\\n**Explanation:** we can do one operation on index 1.\\nThe array after the operation is \\\\[3,-2,6,-5\\\\]. The prefix sum array is \\\\[3, 1, 7, 2\\\\].\\n\\n**Constraints:**\\n\\n*   `1 <= nums.length <= 105`\\n*   `-109 <= nums[i] <= 109`\\n```python\\ndef min_operations(nums):\\n    min_prefix = 0\\n    prefix = 0\\n    for num in nums:\\n        prefix += num\\n        min_prefix = min(min_prefix, prefix)\\n    return -min_prefix if min_prefix < 0 else 0\\n```\\n\\n'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train']['code_with_problem'][2345]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b2e9bbc4-efe4-4edb-acac-62d30f23bd87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'```python\\ndef twoSum(nums, target):\\n    map = {}\\n    for i, num in enumerate(nums):\\n        complement = target - num\\n        if complement in map:\\n            return [map[complement], i]\\n        map[num] = i\\n    return []\\n```\\n\\n'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train']['code_only'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1c8ceb71-a2ef-4621-87b5-dafa3effe268",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The algorithm leverages a hash map (unordered_map in C++, HashMap in Java, dictionary in Python, and Map in JavaScript). It iterates through the given 'nums' array and calculates the complementary value (target - current value). If the complementary value is already in the hash map, it means that we found a solution, and we return those indices. If the complement is not in the hash map, we store the current element in the hash map with its index. If the algorithm doesn't find the solution, it returns an empty array or throws an exception (in Java).\\n\\nThis approach has a time complexity of O(n) and a space complexity of O(n) as well.\""
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train']['explanation_only'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b92d4d6-7006-45ac-a48e-7e2c2e7af27d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Polycarp has $n$ different binary words. A word called binary if it contains only characters \\'0\\' and \\'1\\'. For example, these words are binary: \"0001\", \"11\", \"0\" and \"0011100\".\\n\\nPolycarp wants to offer his set of $n$ binary words to play a game \"words\". In this game, players name words and each next word (starting from the second) must start with the last character of the previous word. The first word can be any. For example, these sequence of words can be named during the game: \"0101\", \"1\", \"10\", \"00\", \"00001\".\\n\\nWord reversal is the operation of reversing the order of the characters. For example, the word \"0111\" after the reversal becomes \"1110\", the word \"11010\" after the reversal becomes \"01011\".\\n\\nProbably, Polycarp has such a set of words that there is no way to put them in the order correspondent to the game rules. In this situation, he wants to reverse some words from his set so that:  the final set of $n$ words still contains different words (i.e. all words are unique);  there is a way to put all words of the final set of words in the order so that the final sequence of $n$ words is consistent with the game rules. \\n\\nPolycarp wants to reverse minimal number of words. Please, help him.\\n\\n\\n-----Input-----\\n\\nThe first line of the input contains one integer $t$ ($1 \\\\le t \\\\le 10^4$) â€” the number of test cases in the input. Then $t$ test cases follow.\\n\\nThe first line of a test case contains one integer $n$ ($1 \\\\le n \\\\le 2\\\\cdot10^5$) â€” the number of words in the Polycarp\\'s set. Next $n$ lines contain these words. All of $n$ words aren\\'t empty and contains only characters \\'0\\' and \\'1\\'. The sum of word lengths doesn\\'t exceed $4\\\\cdot10^6$. All words are different.\\n\\nGuaranteed, that the sum of $n$ for all test cases in the input doesn\\'t exceed $2\\\\cdot10^5$. Also, guaranteed that the sum of word lengths for all test cases in the input doesn\\'t exceed $4\\\\cdot10^6$.\\n\\n\\n-----Output-----\\n\\nPrint answer for all of $t$ test cases in the order they appear.\\n\\nIf there is no answer for the test case, print -1. Otherwise, the first line of the output should contain $k$ ($0 \\\\le k \\\\le n$) â€” the minimal number of words in the set which should be reversed. The second line of the output should contain $k$ distinct integers â€” the indexes of the words in the set which should be reversed. Words are numerated from $1$ to $n$ in the order they appear. If $k=0$ you can skip this line (or you can print an empty line). If there are many answers you can print any of them.\\n\\n\\n-----Example-----\\nInput\\n4\\n4\\n0001\\n1000\\n0011\\n0111\\n3\\n010\\n101\\n0\\n2\\n00000\\n00001\\n4\\n01\\n001\\n0001\\n00001\\n\\nOutput\\n1\\n3 \\n-1\\n0\\n\\n2\\n1 2'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train']['question'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d4f1f019-6cdb-4491-8572-73594bbf6534",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['problem_id', 'question', 'solutions', 'input_output', 'difficulty', 'url', 'starter_code'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['problem_id', 'question', 'solutions', 'input_output', 'difficulty', 'url', 'starter_code'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "10fb3e52-4947-4815-9cb2-2627468f2a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset['train']\n",
    "for q in dataset:\n",
    "     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f068987c-798a-4bba-bcd0-1201821dec67",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'problem_id': 0,\n",
       " 'question': 'Polycarp has $n$ different binary words. A word called binary if it contains only characters \\'0\\' and \\'1\\'. For example, these words are binary: \"0001\", \"11\", \"0\" and \"0011100\".\\n\\nPolycarp wants to offer his set of $n$ binary words to play a game \"words\". In this game, players name words and each next word (starting from the second) must start with the last character of the previous word. The first word can be any. For example, these sequence of words can be named during the game: \"0101\", \"1\", \"10\", \"00\", \"00001\".\\n\\nWord reversal is the operation of reversing the order of the characters. For example, the word \"0111\" after the reversal becomes \"1110\", the word \"11010\" after the reversal becomes \"01011\".\\n\\nProbably, Polycarp has such a set of words that there is no way to put them in the order correspondent to the game rules. In this situation, he wants to reverse some words from his set so that:  the final set of $n$ words still contains different words (i.e. all words are unique);  there is a way to put all words of the final set of words in the order so that the final sequence of $n$ words is consistent with the game rules. \\n\\nPolycarp wants to reverse minimal number of words. Please, help him.\\n\\n\\n-----Input-----\\n\\nThe first line of the input contains one integer $t$ ($1 \\\\le t \\\\le 10^4$) â€” the number of test cases in the input. Then $t$ test cases follow.\\n\\nThe first line of a test case contains one integer $n$ ($1 \\\\le n \\\\le 2\\\\cdot10^5$) â€” the number of words in the Polycarp\\'s set. Next $n$ lines contain these words. All of $n$ words aren\\'t empty and contains only characters \\'0\\' and \\'1\\'. The sum of word lengths doesn\\'t exceed $4\\\\cdot10^6$. All words are different.\\n\\nGuaranteed, that the sum of $n$ for all test cases in the input doesn\\'t exceed $2\\\\cdot10^5$. Also, guaranteed that the sum of word lengths for all test cases in the input doesn\\'t exceed $4\\\\cdot10^6$.\\n\\n\\n-----Output-----\\n\\nPrint answer for all of $t$ test cases in the order they appear.\\n\\nIf there is no answer for the test case, print -1. Otherwise, the first line of the output should contain $k$ ($0 \\\\le k \\\\le n$) â€” the minimal number of words in the set which should be reversed. The second line of the output should contain $k$ distinct integers â€” the indexes of the words in the set which should be reversed. Words are numerated from $1$ to $n$ in the order they appear. If $k=0$ you can skip this line (or you can print an empty line). If there are many answers you can print any of them.\\n\\n\\n-----Example-----\\nInput\\n4\\n4\\n0001\\n1000\\n0011\\n0111\\n3\\n010\\n101\\n0\\n2\\n00000\\n00001\\n4\\n01\\n001\\n0001\\n00001\\n\\nOutput\\n1\\n3 \\n-1\\n0\\n\\n2\\n1 2',\n",
       " 'solutions': '[\"for _ in range(int(input())):\\\\n    n = int(input())\\\\n    mass = []\\\\n    zo = 0\\\\n    oz = 0\\\\n    zz = 0\\\\n    oo = 0\\\\n    ozs = []\\\\n    zos = []\\\\n    ozss = set()\\\\n    zoss = set()\\\\n    for j in range(n):\\\\n        k = input()\\\\n        mass.append(k)\\\\n        if k[0] == \\'0\\' and k[-1] == \\'1\\':\\\\n            zoss.add(k)\\\\n            zos.append(j + 1)\\\\n            zo += 1\\\\n        elif k[0] == \\'1\\' and k[-1] == \\'0\\':\\\\n            ozss.add(k)\\\\n            ozs.append(j + 1)\\\\n            oz += 1\\\\n        elif k[0] == \\'0\\' and k[-1] == \\'0\\':\\\\n            zz += 1\\\\n        else:\\\\n            oo += 1\\\\n    if zz and oo and not oz and not zo:\\\\n        print(-1)\\\\n        continue\\\\n    else:\\\\n        if zo > oz:\\\\n            print((zo - oz) // 2)\\\\n            ans = []\\\\n            need = (zo - oz) // 2\\\\n            i = 0\\\\n            while need:\\\\n                zzz = mass[zos[i] - 1][len(mass[zos[i] - 1]) - 1:: -1]\\\\n                if zzz not in ozss:\\\\n                    ans.append(zos[i])\\\\n                    need -= 1\\\\n                i += 1\\\\n            print(*ans)\\\\n        else:\\\\n            print((oz - zo) // 2)\\\\n            ans = []\\\\n            need = (oz - zo) // 2\\\\n            i = 0\\\\n            while need:\\\\n                zzz = mass[ozs[i] - 1][len(mass[ozs[i] - 1]) - 1:: -1]\\\\n                if zzz not in zoss:\\\\n                    ans.append(ozs[i])\\\\n                    need -= 1\\\\n                i += 1\\\\n            print(*ans)\\\\n\", \"k = int(input())\\\\nfor i in range(k):\\\\n    is_t = set()\\\\n    a = dict()\\\\n    a[\\'00\\'] = []\\\\n    a[\\'11\\'] = []\\\\n    a[\\'01\\'] = []\\\\n    a[\\'10\\'] = []    \\\\n    n = int(input())\\\\n    s = []\\\\n    for i in range(n):\\\\n        b = input()\\\\n        a[b[0] + b[-1]].append(i)\\\\n        s.append(b)\\\\n        is_t.add(b)\\\\n    c = len(a[\\'10\\'])\\\\n    d = len(a[\\'01\\'])\\\\n    if c + d == 0:\\\\n        if len(a[\\'00\\']) == 0 or len(a[\\'11\\']) == 0:\\\\n            print(0)\\\\n        else:\\\\n            print(-1)\\\\n    elif c > d:\\\\n        ans = []\\\\n        i = 0\\\\n        m = (d + c) // 2\\\\n        while d != m and i < len(a[\\'10\\']):\\\\n            s1 = s[a[\\'10\\'][i]]\\\\n            if s1[::-1] not in is_t:\\\\n                d += 1\\\\n                ans.append(a[\\'10\\'][i] + 1)\\\\n            i += 1\\\\n        if d != m:\\\\n            print(-1)\\\\n        else:\\\\n            print(len(ans))\\\\n            print(*ans)\\\\n    else:\\\\n        ans = []\\\\n        i = 0\\\\n        m = (d + c) // 2\\\\n        while c != m and i < len(a[\\'01\\']):\\\\n            s1 = s[a[\\'01\\'][i]]\\\\n            if s1[::-1] not in is_t:\\\\n                c += 1\\\\n                ans.append(a[\\'01\\'][i] + 1)\\\\n            i += 1\\\\n        if c != m:\\\\n            print(-1)\\\\n        else:\\\\n            print(len(ans))\\\\n            print(*ans)\\\\n\", \"N = int(input())\\\\n\\\\ndef ceildiv(x, y):\\\\n    if x % y == 0:\\\\n        return x // y\\\\n    else:\\\\n        return x // y + 1\\\\n\\\\nfor _ in range(N):\\\\n    doms = []\\\\n    oc, zc = 0, 0\\\\n    n = int(input())\\\\n\\\\n    used = set()\\\\n    fulls = dict()\\\\n\\\\n    for i in range(n):\\\\n        d = input()\\\\n        used.add(d)\\\\n        if d[0] != d[-1]:\\\\n            fulls[i] = d\\\\n            doms.append((i, (d[0], d[-1])))\\\\n        else:\\\\n            if d[0] == \\'0\\':\\\\n                zc = 1\\\\n            else:\\\\n                oc = 1\\\\n\\\\n    if len(doms) == 0:\\\\n        if zc == 1 and oc == 1:\\\\n            print(-1)\\\\n        else:\\\\n            print(0)\\\\n    else:\\\\n        # print(doms)\\\\n\\\\n        _01 = 0\\\\n        _10 = 0\\\\n\\\\n        _01_indexes = []\\\\n        _10_indexes = []\\\\n\\\\n\\\\n        for dom in doms:\\\\n            if dom[1] == (\\'0\\', \\'1\\'):\\\\n                _01 += 1\\\\n                _01_indexes.append(dom[0])\\\\n            else:\\\\n                _10 += 1\\\\n                _10_indexes.append(dom[0])\\\\n\\\\n        if _10 < _01:\\\\n            _01, _10 = _10, _01\\\\n            _01_indexes, _10_indexes = _10_indexes, _01_indexes\\\\n\\\\n        _10_indexes = [x for x in _10_indexes if fulls[x][::-1] not in used] \\\\n\\\\n        need = ceildiv(_10-_01-1, 2)\\\\n        if len(_10_indexes) >= need:\\\\n            print(need)\\\\n            print( \\' \\'.join(list([str(x+1) for x in _10_indexes[:need]])) )\\\\n        else:\\\\n            print(-1)\\\\n\\\\n    # print(\\\\\"===\\\\\")\\\\n        # print(ceil(abs(doms.count((\\'0\\', \\'1\\')) - doms.count((\\'1\\', \\'0\\'))) - 1, 2))\\\\n\\\\n\", \"t=int(input())\\\\nfor _ in range(t):\\\\n    n=int(input())\\\\n    k={\\\\\"01\\\\\":0,\\\\\"00\\\\\":0,\\\\\"11\\\\\":0,\\\\\"10\\\\\":0}\\\\n    ab=[]\\\\n    ba=[]\\\\n    a=[]\\\\n    ra=set()\\\\n    rb=set()\\\\n    for i in range(n):\\\\n        s=input()\\\\n        ts=s[0]+s[-1]\\\\n        k[ts]+=1\\\\n        if ts==\\\\\"01\\\\\":\\\\n            ab.append([str(i+1),s])\\\\n            ra.add(s)\\\\n        if ts==\\\\\"10\\\\\":\\\\n            ba.append([str(i+1),s])\\\\n            rb.add(s)\\\\n    if k[\\\\\"01\\\\\"]==0 and k[\\\\\"10\\\\\"]==0 and k[\\\\\"00\\\\\"]>0 and k[\\\\\"11\\\\\"]>0:\\\\n        ans=-1\\\\n    else:\\\\n        if k[\\\\\"01\\\\\"]==k[\\\\\"10\\\\\"] or k[\\\\\"01\\\\\"]==k[\\\\\"10\\\\\"]+1 or k[\\\\\"01\\\\\"]==k[\\\\\"10\\\\\"]-1:\\\\n            ans=0\\\\n        else:\\\\n            m=(k[\\\\\"01\\\\\"]+k[\\\\\"10\\\\\"])//2 if (k[\\\\\"01\\\\\"]+k[\\\\\"10\\\\\"])%2==0 else (k[\\\\\"01\\\\\"]+k[\\\\\"10\\\\\"])//2+1\\\\n            if k[\\\\\"01\\\\\"]>m:\\\\n                ans=k[\\\\\"01\\\\\"]-m\\\\n                for i in range(len(ab)):\\\\n                    psp=ab[i][1]\\\\n                    nn=list(psp)\\\\n                    nn.reverse()\\\\n                    psp=\\\\\"\\\\\".join(nn)\\\\n                    c1=len(rb)\\\\n                    rb.add(psp)\\\\n                    c2=len(rb)\\\\n                    if c1!=c2:\\\\n                        a.append(ab[i][0])\\\\n                if len(a)>=ans:\\\\n                    a=a[:ans]\\\\n                else:\\\\n                    ans=-1\\\\n            else:\\\\n                ans=k[\\\\\"10\\\\\"]-m\\\\n                for i in range(len(ba)):\\\\n                    psp=ba[i][1]\\\\n                    nn=list(psp)\\\\n                    nn.reverse()\\\\n                    psp=\\\\\"\\\\\".join(nn)\\\\n                    c1=len(ra)\\\\n                    ra.add(psp)\\\\n                    c2=len(ra)\\\\n                    if c1!=c2:\\\\n                        a.append(ba[i][0])\\\\n                if len(a)>=ans:\\\\n                    a=a[:ans]\\\\n                else:\\\\n                    ans=-1\\\\n    print(ans)\\\\n    if ans>0:\\\\n        print(\\\\\" \\\\\".join(a))\\\\n\", \"t=int(input())\\\\nfor i in range(t):\\\\n    n=int(input())\\\\n    i0,i1=[],[]\\\\n    l0,l1=[],[]\\\\n    h0,h1=False,False\\\\n    for i in range(n):\\\\n        t=input()\\\\n        if t[0]==\\'0\\' and t[-1]==\\'1\\':\\\\n            i0.append(i)\\\\n            l0.append(t)\\\\n        elif t[0]==\\'1\\' and t[-1]==\\'0\\':\\\\n            i1.append(i)\\\\n            l1.append(t)\\\\n        elif t[0]==t[-1]==\\'1\\':\\\\n            h1=True\\\\n        elif t[0]==t[-1]==\\'0\\':\\\\n            h0=True\\\\n    c0,c1=len(l0),len(l1)\\\\n    req,sl=0,[]\\\\n    s0=set(l0)\\\\n    s1=set(l1)\\\\n    if c0>0 or c1>0:\\\\n        if c0-c1>1:\\\\n            req=(c0-c1)//2\\\\n            sel=0\\\\n            sl=[]\\\\n            for tt in range(len(l0)):\\\\n                t=l0[tt]\\\\n                if not t[::-1] in s1:\\\\n                    req-=1\\\\n                    sl.append(i0[tt]+1)\\\\n                if req==0:\\\\n                    break\\\\n        elif c1-c0>1:\\\\n            req=(c1-c0)//2\\\\n            sel=0\\\\n            sl=[]\\\\n            for tt in range(len(l1)):\\\\n                t=l1[tt]\\\\n                if not t[::-1] in s0:\\\\n                    req-=1\\\\n                    sl.append(i1[tt]+1)\\\\n                if req==0:\\\\n                    break\\\\n        if req>0:\\\\n            print(-1)\\\\n        else:\\\\n            print(len(sl))\\\\n            print(*sl)\\\\n    else:\\\\n        if h0 and h1:\\\\n            print(-1)\\\\n        else:\\\\n            print(0)\\\\n            print(*[])\\\\n\"]',\n",
       " 'input_output': '{\\n  \"inputs\": [\\n    \"4\\\\n4\\\\n0001\\\\n1000\\\\n0011\\\\n0111\\\\n3\\\\n010\\\\n101\\\\n0\\\\n2\\\\n00000\\\\n00001\\\\n4\\\\n01\\\\n001\\\\n0001\\\\n00001\\\\n\"\\n  ],\\n  \"outputs\": [\\n    \"1\\\\n3 \\\\n-1\\\\n0\\\\n\\\\n2\\\\n1 2 \\\\n\"\\n  ]\\n}',\n",
       " 'difficulty': 'interview',\n",
       " 'url': 'https://codeforces.com/problemset/problem/1259/D',\n",
       " 'starter_code': ''}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "44ee0fee-03f1-4eb9-a622-6aba2ececd41",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'solution'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mq\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msolution\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'solution'"
     ]
    }
   ],
   "source": [
    "q['solution']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d25564b4-5b3f-4d95-b7dd-9f2ceee9177f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"for _ in range(int(input())):\\n    n = int(input())\\n    mass = []\\n    zo = 0\\n    oz = 0\\n    zz = 0\\n    oo = 0\\n    ozs = []\\n    zos = []\\n    ozss = set()\\n    zoss = set()\\n    for j in range(n):\\n        k = input()\\n        mass.append(k)\\n        if k[0] == '0' and k[-1] == '1':\\n            zoss.add(k)\\n            zos.append(j + 1)\\n            zo += 1\\n        elif k[0] == '1' and k[-1] == '0':\\n            ozss.add(k)\\n            ozs.append(j + 1)\\n            oz += 1\\n        elif k[0] == '0' and k[-1] == '0':\\n            zz += 1\\n        else:\\n            oo += 1\\n    if zz and oo and not oz and not zo:\\n        print(-1)\\n        continue\\n    else:\\n        if zo > oz:\\n            print((zo - oz) // 2)\\n            ans = []\\n            need = (zo - oz) // 2\\n            i = 0\\n            while need:\\n                zzz = mass[zos[i] - 1][len(mass[zos[i] - 1]) - 1:: -1]\\n                if zzz not in ozss:\\n                    ans.append(zos[i])\\n                    need -= 1\\n                i += 1\\n            print(*ans)\\n        else:\\n            print((oz - zo) // 2)\\n            ans = []\\n            need = (oz - zo) // 2\\n            i = 0\\n            while need:\\n                zzz = mass[ozs[i] - 1][len(mass[ozs[i] - 1]) - 1:: -1]\\n                if zzz not in zoss:\\n                    ans.append(ozs[i])\\n                    need -= 1\\n                i += 1\\n            print(*ans)\\n\", \"k = int(input())\\nfor i in range(k):\\n    is_t = set()\\n    a = dict()\\n    a['00'] = []\\n    a['11'] = []\\n    a['01'] = []\\n    a['10'] = []    \\n    n = int(input())\\n    s = []\\n    for i in range(n):\\n        b = input()\\n        a[b[0] + b[-1]].append(i)\\n        s.append(b)\\n        is_t.add(b)\\n    c = len(a['10'])\\n    d = len(a['01'])\\n    if c + d == 0:\\n        if len(a['00']) == 0 or len(a['11']) == 0:\\n            print(0)\\n        else:\\n            print(-1)\\n    elif c > d:\\n        ans = []\\n        i = 0\\n        m = (d + c) // 2\\n        while d != m and i < len(a['10']):\\n            s1 = s[a['10'][i]]\\n            if s1[::-1] not in is_t:\\n                d += 1\\n                ans.append(a['10'][i] + 1)\\n            i += 1\\n        if d != m:\\n            print(-1)\\n        else:\\n            print(len(ans))\\n            print(*ans)\\n    else:\\n        ans = []\\n        i = 0\\n        m = (d + c) // 2\\n        while c != m and i < len(a['01']):\\n            s1 = s[a['01'][i]]\\n            if s1[::-1] not in is_t:\\n                c += 1\\n                ans.append(a['01'][i] + 1)\\n            i += 1\\n        if c != m:\\n            print(-1)\\n        else:\\n            print(len(ans))\\n            print(*ans)\\n\", \"N = int(input())\\n\\ndef ceildiv(x, y):\\n    if x % y == 0:\\n        return x // y\\n    else:\\n        return x // y + 1\\n\\nfor _ in range(N):\\n    doms = []\\n    oc, zc = 0, 0\\n    n = int(input())\\n\\n    used = set()\\n    fulls = dict()\\n\\n    for i in range(n):\\n        d = input()\\n        used.add(d)\\n        if d[0] != d[-1]:\\n            fulls[i] = d\\n            doms.append((i, (d[0], d[-1])))\\n        else:\\n            if d[0] == '0':\\n                zc = 1\\n            else:\\n                oc = 1\\n\\n    if len(doms) == 0:\\n        if zc == 1 and oc == 1:\\n            print(-1)\\n        else:\\n            print(0)\\n    else:\\n        # print(doms)\\n\\n        _01 = 0\\n        _10 = 0\\n\\n        _01_indexes = []\\n        _10_indexes = []\\n\\n\\n        for dom in doms:\\n            if dom[1] == ('0', '1'):\\n                _01 += 1\\n                _01_indexes.append(dom[0])\\n            else:\\n                _10 += 1\\n                _10_indexes.append(dom[0])\\n\\n        if _10 < _01:\\n            _01, _10 = _10, _01\\n            _01_indexes, _10_indexes = _10_indexes, _01_indexes\\n\\n        _10_indexes = [x for x in _10_indexes if fulls[x][::-1] not in used] \\n\\n        need = ceildiv(_10-_01-1, 2)\\n        if len(_10_indexes) >= need:\\n            print(need)\\n            print( ' '.join(list([str(x+1) for x in _10_indexes[:need]])) )\\n        else:\\n            print(-1)\\n\\n    # print(\\\"===\\\")\\n        # print(ceil(abs(doms.count(('0', '1')) - doms.count(('1', '0'))) - 1, 2))\\n\\n\", \"t=int(input())\\nfor _ in range(t):\\n    n=int(input())\\n    k={\\\"01\\\":0,\\\"00\\\":0,\\\"11\\\":0,\\\"10\\\":0}\\n    ab=[]\\n    ba=[]\\n    a=[]\\n    ra=set()\\n    rb=set()\\n    for i in range(n):\\n        s=input()\\n        ts=s[0]+s[-1]\\n        k[ts]+=1\\n        if ts==\\\"01\\\":\\n            ab.append([str(i+1),s])\\n            ra.add(s)\\n        if ts==\\\"10\\\":\\n            ba.append([str(i+1),s])\\n            rb.add(s)\\n    if k[\\\"01\\\"]==0 and k[\\\"10\\\"]==0 and k[\\\"00\\\"]>0 and k[\\\"11\\\"]>0:\\n        ans=-1\\n    else:\\n        if k[\\\"01\\\"]==k[\\\"10\\\"] or k[\\\"01\\\"]==k[\\\"10\\\"]+1 or k[\\\"01\\\"]==k[\\\"10\\\"]-1:\\n            ans=0\\n        else:\\n            m=(k[\\\"01\\\"]+k[\\\"10\\\"])//2 if (k[\\\"01\\\"]+k[\\\"10\\\"])%2==0 else (k[\\\"01\\\"]+k[\\\"10\\\"])//2+1\\n            if k[\\\"01\\\"]>m:\\n                ans=k[\\\"01\\\"]-m\\n                for i in range(len(ab)):\\n                    psp=ab[i][1]\\n                    nn=list(psp)\\n                    nn.reverse()\\n                    psp=\\\"\\\".join(nn)\\n                    c1=len(rb)\\n                    rb.add(psp)\\n                    c2=len(rb)\\n                    if c1!=c2:\\n                        a.append(ab[i][0])\\n                if len(a)>=ans:\\n                    a=a[:ans]\\n                else:\\n                    ans=-1\\n            else:\\n                ans=k[\\\"10\\\"]-m\\n                for i in range(len(ba)):\\n                    psp=ba[i][1]\\n                    nn=list(psp)\\n                    nn.reverse()\\n                    psp=\\\"\\\".join(nn)\\n                    c1=len(ra)\\n                    ra.add(psp)\\n                    c2=len(ra)\\n                    if c1!=c2:\\n                        a.append(ba[i][0])\\n                if len(a)>=ans:\\n                    a=a[:ans]\\n                else:\\n                    ans=-1\\n    print(ans)\\n    if ans>0:\\n        print(\\\" \\\".join(a))\\n\", \"t=int(input())\\nfor i in range(t):\\n    n=int(input())\\n    i0,i1=[],[]\\n    l0,l1=[],[]\\n    h0,h1=False,False\\n    for i in range(n):\\n        t=input()\\n        if t[0]=='0' and t[-1]=='1':\\n            i0.append(i)\\n            l0.append(t)\\n        elif t[0]=='1' and t[-1]=='0':\\n            i1.append(i)\\n            l1.append(t)\\n        elif t[0]==t[-1]=='1':\\n            h1=True\\n        elif t[0]==t[-1]=='0':\\n            h0=True\\n    c0,c1=len(l0),len(l1)\\n    req,sl=0,[]\\n    s0=set(l0)\\n    s1=set(l1)\\n    if c0>0 or c1>0:\\n        if c0-c1>1:\\n            req=(c0-c1)//2\\n            sel=0\\n            sl=[]\\n            for tt in range(len(l0)):\\n                t=l0[tt]\\n                if not t[::-1] in s1:\\n                    req-=1\\n                    sl.append(i0[tt]+1)\\n                if req==0:\\n                    break\\n        elif c1-c0>1:\\n            req=(c1-c0)//2\\n            sel=0\\n            sl=[]\\n            for tt in range(len(l1)):\\n                t=l1[tt]\\n                if not t[::-1] in s0:\\n                    req-=1\\n                    sl.append(i1[tt]+1)\\n                if req==0:\\n                    break\\n        if req>0:\\n            print(-1)\\n        else:\\n            print(len(sl))\\n            print(*sl)\\n    else:\\n        if h0 and h1:\\n            print(-1)\\n        else:\\n            print(0)\\n            print(*[])\\n\"]\n"
     ]
    }
   ],
   "source": [
    "print(dataset['train']['solutions'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75831e92-dfc7-4b50-a5be-38fea730d05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir solve_now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9ab340c-6bb3-443b-bd77-5d47abf16f57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Failed to call git rev-parse --git-dir: exit status 128 \n",
      "Git LFS initialized.\n",
      "Cloning into 'CodeLlama-7b-hf'...\n",
      "remote: Enumerating objects: 91, done.\u001b[K\n",
      "remote: Counting objects: 100% (12/12), done.\u001b[K\n",
      "remote: Compressing objects: 100% (4/4), done.\u001b[K\n",
      "remote: Total 91 (delta 10), reused 8 (delta 8), pack-reused 79\u001b[K\n",
      "Unpacking objects: 100% (91/91), 500.72 KiB | 2.25 MiB/s, done.\n",
      "Filtering content: 100% (6/6), 9.10 GiB | 18.29 MiB/s, done.\n",
      "Encountered 3 file(s) that may not have been copied correctly on Windows:\n",
      "\tpytorch_model-00001-of-00003.bin\n",
      "\tpytorch_model-00002-of-00003.bin\n",
      "\tmodel-00001-of-00002.safetensors\n",
      "\n",
      "See: `git lfs help smudge` for more details.\n"
     ]
    }
   ],
   "source": [
    "!git lfs install\n",
    "!git clone https://huggingface.co/codellama/CodeLlama-7b-hf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00ceb51a-c5ed-484e-ba01-9a30d4251463",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf CodeLlama-7b-hf/.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d345f4ba-260a-4ac8-9731-0bc24dd963e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-09-08 17:25:06,914] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:01<00:00,  1.02it/s]\n",
      "trainable params: 67,108,864 || all params: 6,805,655,552 || trainable%: 0.9860749414548096\n",
      "/home/alexw/.local/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexwortega\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.10 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/mnt/opt/alexw/Experements/SolveITnow/wandb/run-20230908_172827-ng1h0uy0\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mswept-pond-338\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/alexwortega/huggingface\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/alexwortega/huggingface/runs/ng1h0uy0\u001b[0m\n",
      "  0%|                                                  | 0/1767 [00:00<?, ?it/s]You're using a CodeLlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "{'loss': 0.6913, 'learning_rate': 2.2471910112359552e-05, 'epoch': 0.02}        \n",
      "{'loss': 0.6691, 'learning_rate': 4.4943820224719104e-05, 'epoch': 0.03}        \n",
      "{'loss': 0.5562, 'learning_rate': 6.741573033707866e-05, 'epoch': 0.05}         \n",
      "{'loss': 0.6168, 'learning_rate': 8.988764044943821e-05, 'epoch': 0.07}         \n",
      "{'loss': 0.5449, 'learning_rate': 0.00011235955056179777, 'epoch': 0.08}        \n",
      "{'loss': 0.5381, 'learning_rate': 0.00013483146067415732, 'epoch': 0.1}         \n",
      "{'loss': 0.4439, 'learning_rate': 0.00015730337078651685, 'epoch': 0.12}        \n",
      "{'loss': 0.3482, 'learning_rate': 0.00017977528089887642, 'epoch': 0.14}        \n",
      "{'loss': 0.3965, 'learning_rate': 0.0001999998247388287, 'epoch': 0.15}         \n",
      "{'loss': 0.3275, 'learning_rate': 0.00019997879414159885, 'epoch': 0.17}        \n",
      "{'loss': 0.3775, 'learning_rate': 0.0001999227197566496, 'epoch': 0.19}         \n",
      "{'loss': 0.3999, 'learning_rate': 0.00019983162123873727, 'epoch': 0.2}         \n",
      "{'loss': 0.3844, 'learning_rate': 0.0001997055305190044, 'epoch': 0.22}         \n",
      "{'loss': 0.3766, 'learning_rate': 0.00019954449179378735, 'epoch': 0.24}        \n",
      "{'loss': 0.3228, 'learning_rate': 0.00019934856150912497, 'epoch': 0.25}        \n",
      "{'loss': 0.3169, 'learning_rate': 0.0001991178083409737, 'epoch': 0.27}         \n",
      "{'loss': 0.3526, 'learning_rate': 0.00019885231317113555, 'epoch': 0.29}        \n",
      "{'loss': 0.3287, 'learning_rate': 0.0001985521690589083, 'epoch': 0.31}         \n",
      "{'loss': 0.309, 'learning_rate': 0.00019821748120846692, 'epoch': 0.32}         \n",
      "{'loss': 0.3708, 'learning_rate': 0.0001978483669319883, 'epoch': 0.34}         \n",
      "{'loss': 0.2755, 'learning_rate': 0.00019744495560853188, 'epoch': 0.36}        \n",
      "{'loss': 0.3796, 'learning_rate': 0.0001970073886386907, 'epoch': 0.37}         \n",
      "{'loss': 0.2905, 'learning_rate': 0.00019653581939502876, 'epoch': 0.39}        \n",
      "{'loss': 0.3563, 'learning_rate': 0.00019603041316832212, 'epoch': 0.41}        \n",
      "{'loss': 0.3115, 'learning_rate': 0.00019549134710962232, 'epoch': 0.42}        \n",
      "{'loss': 0.3212, 'learning_rate': 0.000194918810168163, 'epoch': 0.44}          \n",
      "{'loss': 0.2817, 'learning_rate': 0.00019431300302513068, 'epoch': 0.46}        \n",
      "{'loss': 0.3952, 'learning_rate': 0.00019367413802332372, 'epoch': 0.47}        \n",
      "{'loss': 0.3583, 'learning_rate': 0.00019300243909272337, 'epoch': 0.49}        \n",
      "{'loss': 0.3806, 'learning_rate': 0.0001922981416720038, 'epoch': 0.51}         \n",
      "{'loss': 0.2604, 'learning_rate': 0.00019156149262600792, 'epoch': 0.53}        \n",
      "{'loss': 0.3543, 'learning_rate': 0.000190792750159218, 'epoch': 0.54}          \n",
      "{'loss': 0.3172, 'learning_rate': 0.00018999218372525224, 'epoch': 0.56}        \n",
      "{'loss': 0.2868, 'learning_rate': 0.0001891600739324177, 'epoch': 0.58}         \n",
      "{'loss': 0.2868, 'learning_rate': 0.00018829671244535376, 'epoch': 0.59}        \n",
      "{'loss': 0.3416, 'learning_rate': 0.00018740240188280013, 'epoch': 0.61}        \n",
      "{'loss': 0.3099, 'learning_rate': 0.00018647745571152515, 'epoch': 0.63}        \n",
      "{'loss': 0.3312, 'learning_rate': 0.0001855221981364521, 'epoch': 0.64}         \n",
      "{'loss': 0.2966, 'learning_rate': 0.00018453696398702119, 'epoch': 0.66}        \n",
      "{'loss': 0.3563, 'learning_rate': 0.00018352209859982788, 'epoch': 0.68}        \n",
      "{'loss': 0.2704, 'learning_rate': 0.00018247795769757818, 'epoch': 0.7}         \n",
      "{'loss': 0.3317, 'learning_rate': 0.00018140490726440342, 'epoch': 0.71}        \n",
      "{'loss': 0.2978, 'learning_rate': 0.00018030332341757837, 'epoch': 0.73}        \n",
      "{'loss': 0.2882, 'learning_rate': 0.00017917359227568772, 'epoch': 0.75}        \n",
      "{'loss': 0.2736, 'learning_rate': 0.0001780161098232865, 'epoch': 0.76}         \n",
      "{'loss': 0.3008, 'learning_rate': 0.0001768312817721029, 'epoch': 0.78}         \n",
      "{'loss': 0.3129, 'learning_rate': 0.0001756195234188312, 'epoch': 0.8}          \n",
      "{'loss': 0.3844, 'learning_rate': 0.00017438125949956537, 'epoch': 0.81}        \n",
      "{'loss': 0.2835, 'learning_rate': 0.00017311692404092376, 'epoch': 0.83}        \n",
      "{'loss': 0.2819, 'learning_rate': 0.0001718269602079175, 'epoch': 0.85}         \n",
      "{'loss': 0.2907, 'learning_rate': 0.0001705118201486157, 'epoch': 0.86}         \n",
      "{'loss': 0.343, 'learning_rate': 0.00016917196483566224, 'epoch': 0.88}         \n",
      "{'loss': 0.2707, 'learning_rate': 0.00016780786390469899, 'epoch': 0.9}         \n",
      "{'loss': 0.3417, 'learning_rate': 0.00016641999548975287, 'epoch': 0.92}        \n",
      "{'loss': 0.2802, 'learning_rate': 0.0001650088460556441, 'epoch': 0.93}         \n",
      "{'loss': 0.3146, 'learning_rate': 0.00016357491022747427, 'epoch': 0.95}        \n",
      "{'loss': 0.3203, 'learning_rate': 0.00016211869061725427, 'epoch': 0.97}        \n",
      "{'loss': 0.3334, 'learning_rate': 0.00016064069764773248, 'epoch': 0.98}        \n",
      "{'loss': 0.3141, 'learning_rate': 0.00015914144937348565, 'epoch': 1.0}         \n",
      "{'loss': 0.3681, 'learning_rate': 0.0001576214712993341, 'epoch': 1.02}         \n",
      "{'loss': 0.3836, 'learning_rate': 0.00015608129619614628, 'epoch': 1.03}        \n",
      "{'loss': 0.3597, 'learning_rate': 0.0001545214639140956, 'epoch': 1.05}         \n",
      "{'loss': 0.2274, 'learning_rate': 0.0001529425211934368, 'epoch': 1.07}         \n",
      "{'loss': 0.2624, 'learning_rate': 0.00015134502147286653, 'epoch': 1.09}        \n",
      "{'loss': 0.3179, 'learning_rate': 0.00014972952469553645, 'epoch': 1.1}         \n",
      "{'loss': 0.2167, 'learning_rate': 0.00014809659711278626, 'epoch': 1.12}        \n",
      "{'loss': 0.2475, 'learning_rate': 0.00014644681108566573, 'epoch': 1.14}        \n",
      "{'loss': 0.3141, 'learning_rate': 0.0001447807448843152, 'epoch': 1.15}         \n",
      "{'loss': 0.334, 'learning_rate': 0.00014309898248527478, 'epoch': 1.17}         \n",
      "{'loss': 0.2916, 'learning_rate': 0.00014140211336679346, 'epoch': 1.19}        \n",
      "{'loss': 0.2861, 'learning_rate': 0.00013969073230220977, 'epoch': 1.2}         \n",
      "{'loss': 0.2706, 'learning_rate': 0.00013796543915147648, 'epoch': 1.22}        \n",
      "{'loss': 0.2902, 'learning_rate': 0.00013622683865090247, 'epoch': 1.24}        \n",
      "{'loss': 0.277, 'learning_rate': 0.00013447554020118506, 'epoch': 1.25}         \n",
      "{'loss': 0.2509, 'learning_rate': 0.00013271215765380773, 'epoch': 1.27}        \n",
      "{'loss': 0.2857, 'learning_rate': 0.00013093730909587782, 'epoch': 1.29}        \n",
      "{'loss': 0.2509, 'learning_rate': 0.00012915161663347918, 'epoch': 1.31}        \n",
      "{'loss': 0.3237, 'learning_rate': 0.00012735570617361668, 'epoch': 1.32}        \n",
      "{'loss': 0.3201, 'learning_rate': 0.00012555020720482794, 'epoch': 1.34}        \n",
      "{'loss': 0.281, 'learning_rate': 0.00012373575257653997, 'epoch': 1.36}         \n",
      "{'loss': 0.2886, 'learning_rate': 0.00012191297827724782, 'epoch': 1.37}        \n",
      "{'loss': 0.3037, 'learning_rate': 0.00012008252321159272, 'epoch': 1.39}        \n",
      "{'loss': 0.3057, 'learning_rate': 0.00011824502897641834, 'epoch': 1.41}        \n",
      "{'loss': 0.2854, 'learning_rate': 0.00011640113963588312, 'epoch': 1.42}        \n",
      "{'loss': 0.3252, 'learning_rate': 0.00011455150149570803, 'epoch': 1.44}        \n",
      "{'loss': 0.3032, 'learning_rate': 0.00011269676287663847, 'epoch': 1.46}        \n",
      "{'loss': 0.2902, 'learning_rate': 0.00011083757388719988, 'epoch': 1.48}        \n",
      "{'loss': 0.2898, 'learning_rate': 0.00010897458619582703, 'epoch': 1.49}        \n",
      "{'loss': 0.2739, 'learning_rate': 0.00010710845280244591, 'epoch': 1.51}        \n",
      "{'loss': 0.2859, 'learning_rate': 0.00010523982780958946, 'epoch': 1.53}        \n",
      "{'loss': 0.2849, 'learning_rate': 0.00010336936619312679, 'epoch': 1.54}        \n",
      "{'loss': 0.2421, 'learning_rate': 0.00010149772357268585, 'epoch': 1.56}        \n",
      "{'loss': 0.2929, 'learning_rate': 9.962555598185102e-05, 'epoch': 1.58}         \n",
      "{'loss': 0.2745, 'learning_rate': 9.775351963821515e-05, 'epoch': 1.59}         \n",
      "{'loss': 0.2871, 'learning_rate': 9.588227071336728e-05, 'epoch': 1.61}         \n",
      "{'loss': 0.2796, 'learning_rate': 9.401246510289671e-05, 'epoch': 1.63}         \n",
      "{'loss': 0.2445, 'learning_rate': 9.214475819649335e-05, 'epoch': 1.64}         \n",
      "{'loss': 0.3299, 'learning_rate': 9.027980464822565e-05, 'epoch': 1.66}         \n",
      "{'loss': 0.3426, 'learning_rate': 8.841825814707653e-05, 'epoch': 1.68}         \n",
      "{'loss': 0.2996, 'learning_rate': 8.656077118781728e-05, 'epoch': 1.7}          \n",
      "{'loss': 0.2811, 'learning_rate': 8.470799484230019e-05, 'epoch': 1.71}         \n",
      "{'loss': 0.2578, 'learning_rate': 8.286057853124993e-05, 'epoch': 1.73}         \n",
      "{'loss': 0.3003, 'learning_rate': 8.101916979663363e-05, 'epoch': 1.75}         \n",
      "{'loss': 0.2421, 'learning_rate': 7.918441407468966e-05, 'epoch': 1.76}         \n",
      "{'loss': 0.3424, 'learning_rate': 7.735695446969411e-05, 'epoch': 1.78}         \n",
      "{'loss': 0.3123, 'learning_rate': 7.553743152854516e-05, 'epoch': 1.8}          \n",
      "{'loss': 0.2909, 'learning_rate': 7.37264830162433e-05, 'epoch': 1.81}          \n",
      "{'loss': 0.3032, 'learning_rate': 7.192474369234708e-05, 'epoch': 1.83}         \n",
      "{'loss': 0.2925, 'learning_rate': 7.01328450884822e-05, 'epoch': 1.85}          \n",
      "{'loss': 0.2763, 'learning_rate': 6.835141528698175e-05, 'epoch': 1.87}         \n",
      "{'loss': 0.3125, 'learning_rate': 6.658107870073585e-05, 'epoch': 1.88}         \n",
      "{'loss': 0.303, 'learning_rate': 6.482245585432726e-05, 'epoch': 1.9}           \n",
      "{'loss': 0.2912, 'learning_rate': 6.307616316652997e-05, 'epoch': 1.92}         \n",
      "{'loss': 0.3004, 'learning_rate': 6.134281273424687e-05, 'epoch': 1.93}         \n",
      "{'loss': 0.2307, 'learning_rate': 5.962301211796228e-05, 'epoch': 1.95}         \n",
      "{'loss': 0.2801, 'learning_rate': 5.7917364128784635e-05, 'epoch': 1.97}        \n",
      "{'loss': 0.311, 'learning_rate': 5.6226466617153804e-05, 'epoch': 1.98}         \n",
      "{'loss': 0.2774, 'learning_rate': 5.455091226328732e-05, 'epoch': 2.0}          \n",
      "{'loss': 0.2551, 'learning_rate': 5.2891288369438455e-05, 'epoch': 2.02}        \n",
      "{'loss': 0.2227, 'learning_rate': 5.124817665403995e-05, 'epoch': 2.03}         \n",
      "{'loss': 0.2978, 'learning_rate': 4.962215304780443e-05, 'epoch': 2.05}         \n",
      "{'loss': 0.2541, 'learning_rate': 4.801378749185357e-05, 'epoch': 2.07}         \n",
      "{'loss': 0.2396, 'learning_rate': 4.6423643737946974e-05, 'epoch': 2.09}        \n",
      "{'loss': 0.2691, 'learning_rate': 4.485227915088023e-05, 'epoch': 2.1}          \n",
      "{'loss': 0.3433, 'learning_rate': 4.33002445131216e-05, 'epoch': 2.12}          \n",
      "{'loss': 0.281, 'learning_rate': 4.176808383175633e-05, 'epoch': 2.14}          \n",
      "{'loss': 0.2617, 'learning_rate': 4.02563341478052e-05, 'epoch': 2.15}          \n",
      "{'loss': 0.2915, 'learning_rate': 3.876552534798534e-05, 'epoch': 2.17}         \n",
      "{'loss': 0.2376, 'learning_rate': 3.729617997897842e-05, 'epoch': 2.19}         \n",
      "{'loss': 0.2752, 'learning_rate': 3.584881306427152e-05, 'epoch': 2.2}          \n",
      "{'loss': 0.2594, 'learning_rate': 3.4423931923635347e-05, 'epoch': 2.22}        \n",
      "{'loss': 0.2962, 'learning_rate': 3.3022035995302133e-05, 'epoch': 2.24}        \n",
      "{'loss': 0.29, 'learning_rate': 3.164361666090681e-05, 'epoch': 2.26}           \n",
      "{'loss': 0.2923, 'learning_rate': 3.0289157073251518e-05, 'epoch': 2.27}        \n",
      "{'loss': 0.2316, 'learning_rate': 2.8959131986954635e-05, 'epoch': 2.29}        \n",
      "{'loss': 0.2623, 'learning_rate': 2.7654007592043672e-05, 'epoch': 2.31}        \n",
      "{'loss': 0.3057, 'learning_rate': 2.6374241350549767e-05, 'epoch': 2.32}        \n",
      "{'loss': 0.2785, 'learning_rate': 2.5120281836161807e-05, 'epoch': 2.34}        \n",
      "{'loss': 0.2166, 'learning_rate': 2.3892568576995833e-05, 'epoch': 2.36}        \n",
      "{'loss': 0.2867, 'learning_rate': 2.2691531901535014e-05, 'epoch': 2.37}        \n",
      "{'loss': 0.2573, 'learning_rate': 2.1517592787794326e-05, 'epoch': 2.39}        \n",
      "{'loss': 0.2358, 'learning_rate': 2.037116271576265e-05, 'epoch': 2.41}         \n",
      "{'loss': 0.2463, 'learning_rate': 1.9252643523173874e-05, 'epoch': 2.42}        \n",
      "{'loss': 0.2326, 'learning_rate': 1.8162427264658044e-05, 'epoch': 2.44}        \n",
      "{'loss': 0.3069, 'learning_rate': 1.710089607432124e-05, 'epoch': 2.46}         \n",
      "{'loss': 0.2475, 'learning_rate': 1.60684220318031e-05, 'epoch': 2.48}          \n",
      "{'loss': 0.2718, 'learning_rate': 1.5065367031858235e-05, 'epoch': 2.49}        \n",
      "{'loss': 0.249, 'learning_rate': 1.40920826575077e-05, 'epoch': 2.51}           \n",
      "{'loss': 0.2552, 'learning_rate': 1.3148910056804953e-05, 'epoch': 2.53}        \n",
      "{'loss': 0.2664, 'learning_rate': 1.2236179823259208e-05, 'epoch': 2.54}        \n",
      "{'loss': 0.2223, 'learning_rate': 1.1354211879958353e-05, 'epoch': 2.56}        \n",
      "{'loss': 0.2916, 'learning_rate': 1.0503315367432153e-05, 'epoch': 2.58}        \n",
      "{'loss': 0.2368, 'learning_rate': 9.683788535294491e-06, 'epoch': 2.59}         \n",
      "{'loss': 0.3181, 'learning_rate': 8.895918637703515e-06, 'epoch': 2.61}         \n",
      "{'loss': 0.2237, 'learning_rate': 8.139981832675259e-06, 'epoch': 2.63}         \n",
      "{'loss': 0.2919, 'learning_rate': 7.416243085287123e-06, 'epoch': 2.65}         \n",
      "{'loss': 0.2839, 'learning_rate': 6.724956074804245e-06, 'epoch': 2.66}         \n",
      "{'loss': 0.2744, 'learning_rate': 6.0663631057618404e-06, 'epoch': 2.68}        \n",
      "{'loss': 0.2303, 'learning_rate': 5.440695023034581e-06, 'epoch': 2.7}          \n",
      "{'loss': 0.2442, 'learning_rate': 4.848171130922563e-06, 'epoch': 2.71}         \n",
      "{'loss': 0.2927, 'learning_rate': 4.2889991162825e-06, 'epoch': 2.73}           \n",
      "{'loss': 0.2539, 'learning_rate': 3.76337497573096e-06, 'epoch': 2.75}          \n",
      "{'loss': 0.2474, 'learning_rate': 3.271482946945126e-06, 'epoch': 2.76}         \n",
      "{'loss': 0.3005, 'learning_rate': 2.81349544408529e-06, 'epoch': 2.78}          \n",
      "{'loss': 0.3093, 'learning_rate': 2.3895729973615443e-06, 'epoch': 2.8}         \n",
      "{'loss': 0.2747, 'learning_rate': 1.9998641967659882e-06, 'epoch': 2.81}        \n",
      "{'loss': 0.2556, 'learning_rate': 1.644505639990146e-06, 'epoch': 2.83}         \n",
      "{'loss': 0.2403, 'learning_rate': 1.3236218845457894e-06, 'epoch': 2.85}        \n",
      "{'loss': 0.3034, 'learning_rate': 1.0373254041060153e-06, 'epoch': 2.87}        \n",
      "{'loss': 0.3028, 'learning_rate': 7.857165490818275e-07, 'epoch': 2.88}         \n",
      "{'loss': 0.2582, 'learning_rate': 5.688835114481373e-07, 'epoch': 2.9}          \n",
      "{'loss': 0.2052, 'learning_rate': 3.8690229383135935e-07, 'epoch': 2.92}        \n",
      "{'loss': 0.2964, 'learning_rate': 2.398366828696097e-07, 'epoch': 2.93}         \n",
      "{'loss': 0.2263, 'learning_rate': 1.2773822685464566e-07, 'epoch': 2.95}        \n",
      "{'loss': 0.2581, 'learning_rate': 5.0646217663608e-08, 'epoch': 2.97}           \n",
      "{'loss': 0.2281, 'learning_rate': 8.587676986737681e-09, 'epoch': 2.98}         \n",
      "{'train_runtime': 2810.1027, 'train_samples_per_second': 2.518, 'train_steps_per_second': 0.629, 'train_loss': 0.3043864601806658, 'epoch': 3.0}\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1767/1767 [46:47<00:00,  1.59s/it]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate â–ƒâ–†â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–…â–…â–…â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss â–ˆâ–†â–ƒâ–ƒâ–ƒâ–ƒâ–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–‚â–â–‚â–‚â–‚â–‚â–â–â–‚â–‚â–â–‚â–‚â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch 3.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step 1767\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss 0.2281\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos 1.7724826015024742e+17\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss 0.30439\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime 2810.1027\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second 2.518\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second 0.629\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run \u001b[33mswept-pond-338\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/alexwortega/huggingface/runs/ng1h0uy0\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230908_172827-ng1h0uy0/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=2 python3 sft.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d45bf8f8-d2fc-4440-9e32-c9dfba853a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "55ba8c2a-1ec7-44a4-aa2c-53228109f817",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer.encode(truncation=)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5612278b-0f89-4b7a-9212-3532b9dca995",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ab97f317f95486895f30b617b5c2187",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2359 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "from datasets import load_dataset\n",
    " \n",
    "dataset = load_dataset(\"json\",data_files= \"leetcode-solutions.json\")\n",
    "\n",
    "import torch\n",
    "from multiprocessing import Process, Queue, Manager\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "class rulm_Dataset(Dataset):\n",
    "    def __init__(self, dataset, tokenizer):\n",
    "        \n",
    "        \n",
    "        i = 1000000000\n",
    "        self.tokenized = []\n",
    "        \n",
    "        for q in tqdm(dataset):\n",
    "                    try:\n",
    "                        if i>0:\n",
    "                            #print(q)\n",
    "                            pr = q[\"code_with_problem\"]\n",
    "                            encoded_pr = self._encode(text=pr, tokenizer=tokenizer)\n",
    "\n",
    "                            i=i-1\n",
    "                            self.tokenized.append(encoded_pr)\n",
    "                        else:\n",
    "                            break\n",
    "                    except Exception as e:\n",
    "                        print(e)\n",
    "                        pass\n",
    "        c = torch.cat(self.tokenized,1)\n",
    "        self.samples = c[0].split(1024)\n",
    "       \n",
    "\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return (self.samples[item])\n",
    "\n",
    "    def _encode(self, text, tokenizer):\n",
    "        encoded_sample = tokenizer.encode(text, add_special_tokens=False,  return_tensors='pt')\n",
    "        return encoded_sample\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "concatenated_dataset = rulm_Dataset(dataset['train'], tokenizer)\n",
    "\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "loader = DataLoader(concatenated_dataset, batch_size=1, num_workers=0, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b5f66a89-c653-471d-be42-25aa77c90553",
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cfb9b915-e4b6-442b-a20f-c222cc9e0e6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "',15,3,20\\\\],\\\\[8,1,20,27,11\\\\],\\\\[9,4,6,2,21\\\\],\\\\[40,9,1,10,6\\\\],\\\\[22,7,4,5,3\\\\]\\\\]\\n**Output:** 3\\n**Explanation:** The grid on the left shows a valid cornered path.\\nIt has a product of 15 \\\\* 20 \\\\* 6 \\\\* 1 \\\\* 10 = 18000 which has 3 trailing zeros.\\nIt can be shown that this is the maximum trailing zeros in the product of a cornered path.\\n\\nThe grid in the middle is not a cornered path as it has more than one turn.\\nThe grid on the right is not a cornered path as it requires a return to a previously visited cell.\\n\\n**Example 2:**\\n\\n**Input:** grid = \\\\[\\\\[4,3,2\\\\],\\\\[7,6,1\\\\],\\\\[8,8,8\\\\]\\\\]\\n**Output:** 0\\n**Explanation:** The grid is shown in the figure above.\\nThere are no cornered paths in the grid that result in a product with a trailing zero.\\n\\n**Constraints:**\\n\\n*   `m == grid.length`\\n*   `n == grid[i].length`\\n*   `1 <= m, n <= 105`\\n*   `1 <= m * n <= 105`\\n*   `1 <= grid[i][j] <= 1000`\\n```python\\ndef can_destroy_all_asteroids(mass, asteroids):\\n    asteroids.sort()\\n    for asteroid in asteroids:\\n        if mass >= asteroid:\\n            mass += asteroid\\n        else:\\n            return False\\n    return True\\n```\\n\\n\\n # You are given a **tree** (i.e. a connected, undirected graph that has no cycles) **rooted** at node `0` consisting of `n` nodes numbered from `0` to `n - 1`. The tree is represented by a **0-indexed** array `parent` of size `n`, where `parent[i]` is the parent of node `i`. Since node `0` is the root, `parent[0] == -1`.\\n\\nYou are also given a string `s` of length `n`, where `s[i]` is the character assigned to node `i`.\\n\\nReturn _the length of the **longest path** in the tree such that no pair of **adjacent** nodes on the path have the same character assigned to them._\\n\\n**Example 1:**\\n\\n**Input:** parent = \\\\[-1,0,0,1,1,2\\\\], s =  \"abacbe \"\\n**Output:** 3\\n**Explanation:** The longest path where each two adjacent nodes have different characters in the tree is the path: 0 -> 1 -> 3. The length of this path is 3, so 3 is returned.\\nIt can be proven that there is no longer path that satisfies the conditions. \\n\\n**Example 2:**\\n\\n**Input:** parent = \\\\[-1,0,0,0\\\\], s =  \"aabc \"\\n**Output:** 3\\n**Explanation:** The longest path where each two adjacent nodes have different characters is the path: 2 -> 0 -> 3. The length of this path is 3, so 3 is returned.\\n\\n**Constraints:**\\n\\n*   `n == parent.length == s.length`\\n*   `1 <= n <= 105`\\n*   `0 <= parent[i] <= n - 1` for all `i >= 1`\\n*   `parent[0] == -1`\\n*   `parent` represents a valid tree.\\n*   `s` consists of only lowercase English letters.\\n```python\\ndef max_invites(favorite):\\n    n = len(favorite)\\n    dp = [0] * n\\n    max_invites = 0\\n\\n    for i in range(n):\\n        dp[i] = 2 if i == favorite[favorite[i]] else 1\\n        max_invites = max(max_invites, dp[i])\\n\\n    return max_invites\\n```\\n\\n\\n # A series of highways connect `n` cities numbered'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(l[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2161b278-3419-49ab-aac2-dbe568127ebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-09-08 11:20:54,132] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c103ef36f4c40b28e4ac0df6d1c3e27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "MODEL_NAME = 'CodeLlama-7b-hf'\n",
    "import torch\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME,  torch_dtype=torch.float16)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33117dcf-2086-4a61-aab3-ca0aa10d9425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Sep  8 11:20:47 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  On   | 00000000:04:00.0 Off |                  Off |\n",
      "| N/A   39C    P0    58W / 250W |  11683MiB / 32768MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-SXM2...  On   | 00000000:06:00.0 Off |                  Off |\n",
      "| N/A   53C    P0   268W / 250W |  15207MiB / 32768MiB |    100%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla V100-SXM2...  On   | 00000000:07:00.0 Off |                  Off |\n",
      "| N/A   28C    P0    42W / 250W |      3MiB / 32768MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla V100-SXM2...  On   | 00000000:08:00.0 Off |                  Off |\n",
      "| N/A   31C    P0    44W / 250W |      3MiB / 32768MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A   1688555      C   ...project_spring/bin/python    11680MiB |\n",
      "|    1   N/A  N/A   1736764      C   /usr/bin/python3                15204MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0582bce3-fad2-4504-8608-2fab1bb2230b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:3'\n",
    "model.to(device)\n",
    "None\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16, lora_alpha=32, lora_dropout=0.05, bias=\"none\"\n",
    ")\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4728ade-a2e8-4a80-b58d-e7dd69ce506c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5aae5be0-a4bc-4984-b688-ee512ae65f02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f8a97882-0464-4fe6-a612-aee5c4563396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "sft_50000      sft_ll2000.pt  sft_ll4000.pt  sft_ll6000.pt\n",
      "sft_ll1000.pt  sft_ll3000.pt  sft_ll5000.pt  sft_ll7000.pt\n"
     ]
    }
   ],
   "source": [
    "ls solve_now/    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eb542619-400a-48cd-a3d1-f8c914af2adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = torch.load(\"solve_now/sft_ll7000.pt\",map_location='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a2ef00f0-6819-4f30-8b48-27bd49c35d55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "eef4de1e-b4f4-42b1-8c1a-af3edd111a1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=3)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ea3f827b-1815-4ec2-94d3-509f0223b671",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen(q, model, k=2):\n",
    "    gen_kwargs = {\n",
    "        \"min_length\": 20,\n",
    "        \"max_new_tokens\": 256,\n",
    "        \"top_k\": 50,\n",
    "        \"top_p\": 0.7,\n",
    "        \"do_sample\": True,  \n",
    "        \"early_stopping\": True,\n",
    "        \"no_repeat_ngram_size\": 2,\n",
    "        \"eos_token_id\": tokenizer.eos_token_id,\n",
    "        \"pad_token_id\": tokenizer.eos_token_id,\n",
    "        \"use_cache\": True,\n",
    "        #\"repetition_penalty\": 1.5,  \n",
    "        #\"length_penalty\": 1.2,  \n",
    "        \"num_beams\": 1,\n",
    "        \"num_return_sequences\": k\n",
    "    }\n",
    "    \n",
    "    t = tokenizer.encode(q, add_special_tokens=False, return_tensors='pt').to(device)\n",
    "    g = model.generate(t, **gen_kwargs)\n",
    "    import gc\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    generated_sequences = tokenizer.batch_decode(g, skip_special_tokens=False)\n",
    "    \n",
    "    return  generated_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "511a8795-47a3-4c71-8208-4a305e6bf30f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0bb4f3ff-3c8a-4e42-93ac-8d048a373498",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexw/.local/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:399: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"sort-colors # Sort Colors # Medium # Given an array `nums` with `n` objects colored red, white, or blue, sort them **[in-place](https://en.wikipedia.org/wiki/In-place_algorithm)** so that objects of the same color are adjacent, with the colors in the order red, white, and blue. We will use the integers `0`, `1`, and `2` to represent the color red, white, and blue, respectively. You must solve this problem without using the library's sort function. **Example 1:** **Input:** nums = \\\\[2,0,2,1,1,0\\\\] **Output:** \\\\[0,0,1,1,2,2\\\\] **Example 2:** **Input:** nums = \\\\[2,0,1\\\\] **Output:** \\\\[0,1,2\\\\] **Constraints:** * `n == nums.length` * `1 <= n <= 300` * `nums[i]` is either `0`, `1`, or `2`. **Follow up:** Could you come up with a one-pass algorithm using only constant extra space? **Related Topics:*** * *Divide and Conquer* * **Similar Questions:**** * [Wiggle Sort II](wiggle-sort-ii.md) *\\n\\n## é¢˜ç›®å¤§æ„\\nè¿™ä¸€é¢˜çš„æ„æ€æ˜¯æŠŠä¸€ä¸ªæ•°ç»„é‡Œé¢çš„ 0ï¼Œ1ï¼Œ2 ä¸‰ä¸ªå…ƒç´ åˆ†åˆ«æ”¾åˆ° å‰åŽä¸¤è¾¹ã€‚\\nè¦æ±‚ä¸èƒ½ä½¿ç”¨æŽ’åºå‡½æ•°ï¼Œåªèƒ½åœ¨åŽŸæ•°</s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s>\",\n",
       " \"sort-colors # Sort Colors # Medium # Given an array `nums` with `n` objects colored red, white, or blue, sort them **[in-place](https://en.wikipedia.org/wiki/In-place_algorithm)** so that objects of the same color are adjacent, with the colors in the order red, white, and blue. We will use the integers `0`, `1`, and `2` to represent the color red, white, and blue, respectively. You must solve this problem without using the library's sort function. **Example 1:** **Input:** nums = \\\\[2,0,2,1,1,0\\\\] **Output:** \\\\[0,0,1,1,2,2\\\\] **Example 2:** **Input:** nums = \\\\[2,0,1\\\\] **Output:** \\\\[0,1,2\\\\] **Constraints:** * `n == nums.length` * `1 <= n <= 300` * `nums[i]` is either `0`, `1`, or `2`. **Follow up:** Could you come up with a one-pass algorithm using only constant extra space?\\n\\n**Solution:*\\n# æ€è·¯ï¼š\\nè¿™é“é¢˜å’Œä¸Šä¸€é¢˜æœ‰äº›ç›¸ä¼¼ï¼Œæˆ‘ä»¬è¦æŠŠ0æŽ’åœ¨æœ€å‰é¢ï¼Œ1æ‰”åœ¨ä¸­é—´ï¼Œ2æ’åˆ°æœ€åŽé¢ã€‚\\n1. éåŽ†ä¸€æ¬¡æ•°ç»„ï¼Œç»Ÿè®¡0çš„ä¸ªæ•°ï¼Œå‡åŽ»1çš„ï¼Œå¾—åˆ°0æœ€å¤§çš„æ•°é‡ï¼Œè¿™é‡Œæ˜¯0æ•°çš„æœ€å°ä½ç½®ï¼Œå¦‚æžœä¸ä¸º0ï¼Œè¯´æ˜Ž0å’Œ1æ•°ç›®ä¸€æ ·ï¼Œåˆ™æœ€ç»ˆ0åœ¨å‰1åœ¨åŽã€‚å¦‚ä¸º 0 ä¸åœ¨é¦–ä½ï¼Œé‚£ä¹ˆ æˆ‘çš„ç­”æ¡ˆ æ˜¯ æœ€å·¦è¾¹çš„ æ²¡æœ‰æ»¡è¶³çš„ä½å­ï¼Œå°±æ˜¯è¯´æœ€æ—©å‡ºçŽ° çš„é‚£ä¸ª ä½ä½ ã€‚ \\n2. i ä»Žå·¦åˆ°å³é€’å¢žï¼Œå½“é“¾è¡¨ä¸­æ•°å€¼ç­‰äºŽ iï¼Œä¸”é˜Ÿåˆ—å¤´çš„å€¼å°äºŽiï¼Œé¡¶æ ˆï¼Œæš‚å­˜ï¼Œç›´åˆ°éšæœºåˆ°\"]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = \"\"\"sort-colors # Sort Colors # Medium # Given an array `nums` with `n` objects colored red, white, or blue, sort them **[in-place](https://en.wikipedia.org/wiki/In-place_algorithm)** so that objects of the same color are adjacent, with the colors in the order red, white, and blue. We will use the integers `0`, `1`, and `2` to represent the color red, white, and blue, respectively. You must solve this problem without using the library's sort function. **Example 1:** **Input:** nums = \\[2,0,2,1,1,0\\] **Output:** \\[0,0,1,1,2,2\\] **Example 2:** **Input:** nums = \\[2,0,1\\] **Output:** \\[0,1,2\\] **Constraints:** * `n == nums.length` * `1 <= n <= 300` * `nums[i]` is either `0`, `1`, or `2`. **Follow up:** Could you come up with a one-pass algorithm using only constant extra space?\"\"\"\n",
    "gen(q,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3605975a-4f5e-4312-a4c3-cec058433bf1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
