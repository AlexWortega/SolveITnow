{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6f73265-c608-41e5-b783-98cc6feed621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-09-08 18:37:56,655] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41e15df2fd524e9eae4a760de392f76f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "MODEL_NAME = 'WizardCoder-Python-7B-V1.0'\n",
    "import torch\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME,  torch_dtype=torch.float16)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d601d68a-be9a-4459-99ee-427bccf1f056",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f667fdf-4b4c-476c-ad15-9fff6387e733",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aec5a2fb-85b5-4a9e-bec3-5094cdc7e61b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32001, 4096, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32001, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda:3'\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f4201f6-1bb1-4b79-a8a0-1928403cad4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "model = PeftModel.from_pretrained(model, 'results-Instrucr/checkpoint-1000')\n",
    "model = model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fac8d723-1ec0-49a6-a1b4-9744a76f2765",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "43428f88-65e0-414c-a0b9-22ec1236b9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen(q, model, k=2):\n",
    "    gen_kwargs = {\n",
    "        \"min_length\": 20,\n",
    "        \"max_new_tokens\": 512,\n",
    "        \"top_k\": 1000,\n",
    "        \"top_p\": 0.97,\n",
    "        \"do_sample\": True,  \n",
    "        #\"early_stopping\": True,\n",
    "        #\"no_repeat_ngram_size\": 2,\n",
    "       # #\"eos_token_id\": tokenizer.eos_token_id,\n",
    "        #\"pad_token_id\": tokenizer.eos_token_id,\n",
    "        #\"use_cache\": True,\n",
    "        #\"repetition_penalty\": 1.5,  \n",
    "        #\"length_penalty\": 1.2,  \n",
    "        #\"num_beams\": 1,\n",
    "        \"num_return_sequences\": k\n",
    "    }\n",
    "    \n",
    "    t = tokenizer.encode(q, add_special_tokens=False, return_tensors='pt').to(device)\n",
    "    g = model.generate(t, **gen_kwargs)\n",
    "    import gc\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    generated_sequences = tokenizer.batch_decode(g, skip_special_tokens=False)\n",
    "    \n",
    "    return  generated_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "03b5b9c4-2074-4030-8994-0b65eaaa347b",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = \"\"\"Below is an instruction that describes a task. Write a python code response that appropriately completes the request. sort-colors \\n\\n###Instruction: # Sort Colors # Medium # Given an array `nums` with `n` objects colored red, white, or blue, sort them **[in-place](https://en.wikipedia.org/wiki/In-place_algorithm)** so that objects of the same color are adjacent, with the colors in the order red, white, and blue. We will use the integers `0`, `1`, and `2` to represent the color red, white, and blue, respectively. You must solve this problem without using the library's sort function. **Example 1:** **Input:** nums = \\[2,0,2,1,1,0\\] **Output:** \\[0,0,1,1,2,2\\] **Example 2:** **Input:** nums = \\[2,0,1\\] **Output:** \\[0,1,2\\] ### Response: python\\n \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954a5fbf-1c4c-4235-9f2a-f098d031e05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = gen(q,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59404648-4e60-4d9b-8aa4-333eab446727",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(s[1].replace(q,''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ad886c23-aa13-48e1-adfe-4dd7800a2b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "nums = [2,0,2,1,1,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5dff1e44-5b7e-484c-9cd3-245d73446477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Error: Failed to call git rev-parse --git-dir: exit status 128 \n",
      "Git LFS initialized.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Cloning into 'CodeLlama-7b-Instruct-hf'...\n",
      "remote: Enumerating objects: 64, done.\u001b[K\n",
      "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
      "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
      "remote: Total 64 (delta 1), reused 0 (delta 0), pack-reused 61\u001b[K\n",
      "Unpacking objects: 100% (64/64), 496.68 KiB | 2.16 MiB/s, done.\n",
      "Filtering content: 100% (6/6), 9.10 GiB | 29.87 MiB/s, done.\n",
      "Encountered 3 file(s) that may not have been copied correctly on Windows:\n",
      "\tpytorch_model-00001-of-00003.bin\n",
      "\tpytorch_model-00002-of-00003.bin\n",
      "\tmodel-00001-of-00002.safetensors\n",
      "\n",
      "See: `git lfs help smudge` for more details.\n"
     ]
    }
   ],
   "source": [
    "!git lfs install\n",
    "!git clone https://huggingface.co/codellama/CodeLlama-7b-Instruct-hf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33106ea-61f9-45be-95dc-1bb31e2a3f46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7ff97f40-6943-492a-bea1-9fcc40dfdb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def minOperations(nums):\n",
    "    prefix_sum = 0\n",
    "    min_prefix_sum = 0\n",
    "    operations = 0\n",
    "\n",
    "    for i in range(len(nums)):\n",
    "        prefix_sum += nums[i]\n",
    "\n",
    "        if prefix_sum < min_prefix_sum:\n",
    "            # Calculate the number of operations needed to make the prefix sum non-negative\n",
    "            operations += abs(min_prefix_sum - prefix_sum)\n",
    "            # Update the minimum prefix sum\n",
    "            min_prefix_sum = prefix_sum\n",
    "\n",
    "    return operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "57cd04f7-a94d-422d-881c-c14bee42ab8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minOperations(nums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4d0261-6d2a-4c7f-8567-1fef5acba268",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sortColors(num_arr: List[int]) -> None:\n",
    "   i, zero, two = 0\n",
    "    \n",
    "   # Three-way partitioning.\n",
    "   for j in range(len(A)):\n",
    "        if A[j] == 7: # some condition\n",
    "            A.insert(i, j)\n",
    "                     i = i + 8\n",
    "             A = A[:i] + A[(i+1):]\n",
    "print(result) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
